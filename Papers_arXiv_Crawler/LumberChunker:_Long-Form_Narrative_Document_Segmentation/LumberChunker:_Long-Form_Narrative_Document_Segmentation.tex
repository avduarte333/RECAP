\begin{abstract}
Modern NLP tasks increasingly rely on dense retrieval methods to access up-to-date and relevant contextual information. We are motivated by the premise that retrieval benefits from segments that can vary in size such that a content’s semantic independence is better captured. We propose LumberChunker, a method leveraging an LLM to dynamically segment documents, which iteratively prompts the LLM to identify the point within a group of sequential passages where the content begins to shift. To evaluate our method, we introduce GutenQA, a benchmark with 3000 ``needle in a haystack'' type of question-answer pairs derived from 100 public domain narrative books available on Project Gutenberg\footnote{Code and Data available at: \url{https://github.com/joaodsmarques/LumberChunker}}. Our experiments show that LumberChunker not only outperforms the most competitive baseline by 7.37\% in retrieval performance (DCG@20) but also that, when integrated into a RAG pipeline, LumberChunker proves to be more effective than other chunking methods and competitive baselines, such as the Gemini 1.5M Pro.
\end{abstract}



\begin{figure*}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{Figures/chunker_pipeline.png}
  \caption{LumberChunker follows a three-step process. First, we segment a document paragraph-wise. Secondly, a group ($G_i$) is created by appending sequential chunks until exceeding a predefined token count $\theta$. Finally, $G_i$ is fed as context to Gemini, which determines the ID where a significant content shift starts to appear, thus defining the start of $G_{i+1}$ and the end of the current chunk. This process is cyclically repeated for the entire document.}
    \label{fig:LumberChunker-Pipeline}
  \label{fig:method_full_pipeline}
\end{figure*}


\section{Introduction}



The rapid expansion of Large Language Models (LLMs) has paved the way for tackling a wide range of tasks, including translation \cite{tower}, summarization \cite{summarization}, and question answering \cite{palm}, among others \cite{xuandong_llms}. However, a significant issue arises when these models are tasked with generating responses based on information they lack, often resulting in "hallucinations" - responses that, while seemingly plausible, are factually incorrect \cite{zhang2023siren}. Given the broad accessibility of these models, less informed users may accept all generated content as accurate, potentially causing severe misinterpretations and adverse consequences, like the recent incident where a lawyer cited fictitious cases produced by ChatGPT \cite{ChatGPT} in court, resulting in sanctions for the lawyer and highlighting the severe risks of unverified AI-generated information \cite{law_hallucination}.

\par


In the field of question answering, where precision and accuracy of information are paramount, Retrieval Augmented Generation (RAG) systems present a viable solution to hallucinations by grounding the model's generation on contextually relevant documents \cite{NLP_Rag_paper}.


\par
One often overlooked part of the RAG pipeline is how textual content is segmented into `chunks', which can significantly impact the dense retrieval quality \cite{llms_distracted_by_irrelevant_content}. Real-world applications tend to simplify this step and consider sentences, paragraphs, or propositions as the usual granularity level \cite{passage_strategies_old,proposition_paper}.
\par
In this paper, we propose LumberChunker, a novel text segmentation method based on the principle that retrieval efficiency improves when content chunks are as independent as possible from one another. This independence is best achieved by allowing chunks to be of dynamic sizes. Given that language models excel at analyzing text, we leverage their capabilities to identify optimal segmentation points. Specifically, we repeatedly instruct a language model to receive a series of continuous paragraphs and determine the precise paragraph within the sequence where the content starts diverging. This approach ensures that each segment is contextually coherent yet distinct from adjacent segments, thereby enhancing the effectiveness of information retrieval.
\par
We introduce a new benchmark: GutenQA, which consists of 100 public domain narrative books manually extracted from Project Gutenberg\footnote{\url{https://www.gutenberg.org/}}. We create 3000 high-quality question-answer pairs from these books to evaluate the impact of LumberChunker on retrieval. Finally, we integrate LumberChunker into a RAG pipeline for a downstream QA task to assess its effect on the accuracy of the generated outputs.



\section{Background}


The retrieval granularity at which a document is segmented plays an essential role as ineffective chunking strategies can lead to chunks with incomplete context or excessive irrelevant information, which damage the performance of retriever models \cite{llms_distracted_chain_of_note}.
\par
Beyond typical granularity levels like sentences or paragraphs \cite{RAG_survey}, other advanced methods can be employed. Recursive character splitting \cite{recursive} segments text based on a hierarchy of separators such as paragraph breaks, new lines, spaces, and individual characters. While this method better respects the document’s structure, it may lack contextual understanding. To address this, semantic-based splitting \cite{semantic_chunking} utilizes embeddings to cluster semantically similar text segments. This approach ensures that chunks maintain meaningful context and coherence by identifying breakpoints based on significant changes in embedding distances.
\par
The recent research by \citet{proposition_paper} introduces a novel retrieval granularity termed \textit{propositions} - minimal textual units, each conveying an individual fact in a clear, self-sufficient natural language format. While this concept is valid for contexts with fact-based information, like Wikipedia, it may be less effective for narrative texts where the flow and contextual continuity play a critical role (as illustrated in Appendix \ref{sec:propositions_example}).
\par
Retrieval granularity is often viewed at the document level, but it can also involve adjusting the query itself. \citet{hyde_paper} suggests the Hypothetical Document Embeddings (HyDE) method, where an LLM transforms the query into a potential answer document.



\section{Methodology}

\subsection{LumberChunker}

Our main proposed contribution is a novel method for document segmentation named LumberChunker, which employs an LLM to dynamically segment documents into semantically independent chunks. Our approach is grounded in the principle that retrieval benefits from segments that can vary in size to better capture the content's semantic independence. This dynamic granularity ensures that each chunk encapsulates a complete, standalone idea, enhancing the relevance and clarity of the retrieved documents. By feeding the LLM a set of sequential passages, LumberChunker autonomously determines the most appropriate points for segmentation. This decision process takes into account the structure and semantics of the text, thereby enabling the creation of chunks that are optimally sized and contextually coherent.
\par
Figure \ref{fig:LumberChunker-Pipeline} displays the overall pipeline of LumberChunker. We start by splitting the target document paragraph-wise, with each paragraph being uniquely identified by an incremental ID number. Each paragraph is sequentially concatenated into a group $G_i$ until its collective token count surpasses a pre-determined threshold, \( \theta \), which is strategically set based on empirical insights, further discussed in \ref{sec:tuning_theta}. The goal is to set $\theta$ large enough to avoid bisecting relevant larger segments while ensuring it is small enough to prevent overwhelming the model with excessive context, which could hinder its reasoning accuracy. The group $G_i$ is given as input to the LLM (we choose Gemini 1.0-Pro \cite{gemini_1.0}), which we instruct to pinpoint the specific paragraph within \( G_i \) where the content is perceived to diverge significantly from the preceding context. This detection marks the end of a chunk. The document keeps being sequentially partitioned into chunks in a cyclical manner, with the starting point of each new \( G_{i+1} \) group being the paragraph identified in the previous iteration. The prompt used is provided in Appendix \ref{sec:lumberchunker_prompt}.

\subsection{GutenQA}

Our proposed benchmark comprises a collection of 100 books sourced from Project Gutenberg. Due to the diverse HTML structures of these books, we extract the content manually. This avoids potential errors associated with automatic text extraction, as discussed in Appendix \ref{sec:manual_extraction}.
\par
We use ChatGPT (\texttt{gpt-3.5-turbo-0125}) to generate questions for each book. Initially, more than 10000 questions are automatically generated, which are then filtered down such that each book has 30 high-quality questions. To evaluate the method's retrieval capabilities, we specifically design questions to be factual and specific, targeting information unlikely to be repeated elsewhere in the text. This selection strategy favors `what,' `when,' and `where' questions over `why' and `how' questions. The prompt used to instruct the model to generate questions, along with statistics about the distribution of question types within the dataset, is provided in Appendix \ref{sec:appendix_test_data_generation}.



\begin{table*}[h]
\centering
\caption{Passage retrieval performance (DCG@$k$ and Recall@$k$) on GutenQA with different granularities on the questions\textsuperscript{\textdagger} and on the retrieval corpus passages. The best scores in each column are highlighted in \textbf{bold}.}
\label{tab:retrieval_results}
\begin{threeparttable}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{@{}lccccccccccc@{}}
\toprule
\multicolumn{1}{c}{} & \multicolumn{5}{c}{\textbf{DCG @ $k$}} &  & \multicolumn{5}{c}{\textbf{Recall @ $k$}} \\ \cmidrule(lr){2-6} \cmidrule(l){8-12}
\multicolumn{1}{c}{} & 1 & 2 & 5 & 10 & 20 &  & 1 & 2 & 5 & 10 & 20 \\ \cmidrule(r){1-6} \cmidrule(l){8-12}
Semantic Chunking & 29.50 & 35.31 & 40.67 & 43.14 & 44.74 &  & 29.50 & 38.70 & 50.60 & 58.21 & 64.51 \\
Paragraph-Level & 36.54 & 42.11 & 45.87 & 47.72 & 49.00 &  & 36.54 & 45.37 & 53.67 & 59.34 & 64.34 \\
Recursive Chunking & 39.04 & 45.37 & 50.66 & 53.25 & 54.72 &  & 39.04 & 49.07 & 60.64 & 68.62 & 74.35 \\
HyDE\textsuperscript{\textdagger} & 33.47 & 39.74 & 45.06 & 48.14 & 49.92 &  & 33.47 & 43.41 & 55.11 & 64.61 & 71.61\\
Proposition-Level & 36.91 & 42.42 & 44.88 & 45.65 & 46.19 &  & 36.91 & 45.64 & 51.04 & 53.41 & 55.54 \\
LumberChunker & \textbf{48.28} & \textbf{54.86} & \textbf{59.37} & \textbf{60.99} & \textbf{62.09} &  & \textbf{48.28} & \textbf{58.71} & \textbf{68.58} & \textbf{73.58} & \textbf{77.92} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{threeparttable}
\end{table*}



\section{Experiments}

We evaluate LumberChunker using a series of diverse experiments. The key questions that guide our experimental evaluation are as follows:
\begin{itemize}[label=•, leftmargin=*]

\item \textbf{What is the optimal threshold for target token count in each LumberChunker prompt?} We analyze how LumberChunker's DCG@$k$ and Recall@$k$ scores are influenced by different prompt lengths $\theta \in [450, 1000]$ tokens.


\item \textbf{Does LumberChunker enhance retrieval?} We evaluate LumberChunker’s ability to locate highly specific information within the documents, as represented by our GutenQA questions. We compare its DCG@$k$ and Recall@$k$ scores against other baseline methods, such as Semantic or Proposition-Level chunking.


\item \textbf{Do LumberChunker chunks enhance generation quality?} It is natural to question whether the increased computational cost of segmenting a document with our method is worthwhile. To address this, we evaluate if LumberChunker chunks improve generation quality in a QA task. For this purpose, we integrate our chunks into a RAG pipeline and create a smaller QA test set comprising 280 questions based on four narrative autobiographies. As these elements are outside the paper's main scope, further details are provided in Appendix \ref{sec:appendix_rag_pipeline_autobiographies}. We compare our approach with other RAG pipeline variants using different chunking techniques, including manually created chunks, which we consider the gold standard for optimal retrieval. We also employ non-RAG baselines like Gemini 1.5 Pro \cite{gemini_1.5}, capable of processing up to 1.5 million input tokens, and a closed-book setup where Gemini Pro relies solely on internal knowledge.

\end{itemize}



\section{Results and Discussion}


\subsection{Context Size}
\label{sec:tuning_theta}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Figures/LumberChunker_Tune_Theta_DCG.png}
    \caption{Optimizing Context Size $\theta$ ($\approx$ number of tokens in the LumberChunker prompt.)}
    \label{fig:dcg_theta_tune}
\end{figure}

Figure \ref{fig:dcg_theta_tune} reveals that among the various thresholds tested, \(\theta = 550\) leads to the best performance, achieving the highest DCG@$k$ scores across all values of \(k\) tested. This indicates that prompts with around 550 tokens optimize the quality of document retrieval by effectively balancing context capture and passage length. Following this, thresholds \(\theta = 450\) and \(\theta = 650\) show similar but slightly lower performances, suggesting that while they are effective, they do not capture the optimal balance as well as \(\theta = 550\). The threshold \(\theta = 1000\) performs the worst, with noticeably lower DCG@$k$ scores. Given that this task requires advanced reasoning, an excessively long prompt may overwhelm the model's capacity to focus on the relevant parts of the input, thus compromising its performance.


\subsection{Main Results}

The results presented in Table \ref{tab:retrieval_results} highlight that for all values of \(k\), LumberChunker consistently outperforms every other baseline both on DCG@$k$ and Recall@$k$ metrics\footnote{Chunks are encoded with \texttt{text-embedding-ada-002} embeddings from OpenAI.}. This is particularly evident at \(k = 20\), where LumberChunker's DCG score reaches 62.09, while the closest competitor, Recursive chunking, only achieves a score of 54.72. Similarly, in terms of Recall@$k$, LumberChunker attains a score of 77.92 at \(k = 20\), compared to Recursive Chunking's 74.35.
\par
A closer examination of the baselines reveals that methods like Paragraph-Level and Semantic chunking fail to scale effectively as $k$ increases, indicating their limitations in maintaining relevance over a larger number of retrieved documents. HyDE, which uses Recursive chunking as its document granularity level, also fails to outperform its simpler counterpart for every value of $k$. This suggests that the additional augmentation layer the HyDE introduces may not be suited for this task.
\par
The scores for Proposition-Level chunking are notably lower than those of LumberChunker. While Proposition-Level chunking excels in contexts with fine-grained, fact-based information, such as Wikipedia text, it is less effective for narrative texts where flow and contextual continuity play a critical role. For details on the segmentation of GutenQA, refer to Appendix \ref{sec:granularity_statistics}.



\subsection{Impact on QA Systems}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{Figures/rag_display_alt.png}
    \caption{QA Accuracy on Autobiographies Test Set.}
    \label{fig:rag_accuracy}
\end{figure}

In Figure \ref{fig:rag_accuracy}, we observe the performance of different QA methodologies applied to several autobiographies. Notably, our proposed LumberChunker, when integrated into the RAG pipeline, demonstrates superior average accuracy compared to most baselines. Particularly, it outperforms again Recursive chunking, which was the most competitive baseline in the retrieval evaluation (Table \ref{tab:retrieval_results}). This reinforces our view that retrieval has a positive impact on accuracy. LumberChunker only falls short of the RAG approach with manual chunks, which is the expected gold standard in this task.



\section{Conclusions}

In this study, we introduce LumberChunker, a novel text segmentation method leveraging LLM textual capabilities to dynamically segment documents into semantically coherent chunks.
\par
We also present GutenQA, a collection of 100 carefully parsed public domain books, segmented with LumberChunker, for which we create 3000 curated question-answer pairs.

\section{Limitations}
Despite LumberChunker demonstrating superior performance compared to all baselines, it should be highlighted that it requires the use of an LLM, which automatically renders it more expensive and slower compared to traditional methods like Recursive chunking (further discussed in Appendix \ref{sec:computational_efficiency}).
\par
LumberChunker is designed for narrative texts, which are somewhat unstructured and benefit from semantic textual interpretation. However, for scenarios with highly structured texts like those in the legal domain, LumberChunker may be an unnecessarily complex solution because it would likely achieve similar segmentations as those employing document-wise structure parsing (like Markdown segmentation), but at a more expensive cost.
\par
Within the present methodology, LumberChunker also faces scalability issues with the length of individual documents and the volume of documents that need to be processed. While each document needs to be processed only once, the iterative nature of prompting the language model to identify segmentation points can become a drawback when dealing with large number of documents.


\section{Ethical Considerations}

This paper focuses on improving text segmentation methods by leveraging existing large language models. Our released dataset, GutenQA, uses only public domain texts, ensuring there are no privacy concerns or handling of sensitive data. Regarding LumberChunker, we do not foresee any direct impact on malicious activities, disinformation, surveillance, or any significant environmental impact beyond the typical computational requirements.
\par
The only ethical consideration we would like to highlight is our extensive use of black box models. Unlike traditional chunking techniques like Recursive Chunking, which are fully transparent and easily reproducible, black box models introduce some uncertainty regarding their outputs. As a result, it is not impossible that our methodology might have some biases we are unaware of.